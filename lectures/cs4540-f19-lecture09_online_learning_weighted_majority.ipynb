{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# L9: Online Learning and Weighted Majority\n",
    "\n",
    "Jake Abernethy\n",
    "\n",
    "Quiz password: opt\n",
    "\n",
    "*Thursday, September 19, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Online Learning\n",
    "\n",
    "It became clear in the early 90s that you could do learning and prediction *without* using statistical tools. One could prove guarantees for sequential algorithms that would hold on any sequence of data.\n",
    "\n",
    "The seminal paper on this topic was *The Weighted Majority Algorithm* by Littlestone and Warmuth. It was a big idea that has been used in many places, and is a core result in the theory of learning.\n",
    "\n",
    "This idea, plus the *Perceptron Algorithm* from Lecture 1, spawned a number of other ideas in ML. In future lectures we'll be exploring Online Convex Optimization and related tools, that can be used to study lots of other ideas as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing as well as the best expert\n",
    "\n",
    "As you saw in your reading, we can imagine a sequential game as follows:\n",
    "\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts $\\hat y^t \\in \\{0,1\\}$\n",
    "    + Nature reveals the *truth* $y^t \\in \\{0,1\\}$\n",
    "    \n",
    "#### Problem\n",
    "\n",
    "Assume there is a *perfect expert*, i.e. $x_i^t = y^t$ for all $t$. Without knowing $i$ in advance, how can you perform *almost* as well? Come up with an algorithm that makes the smallest number of mistakes possible. (A mistake occurs when $\\hat y^t \\ne y^t$). Note: the Algorithm should make significantly fewer than $N$ mistakes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Halving Algorithm\n",
    "\n",
    "The halving algorithm:\n",
    "+ maintain a pool $C^t \\subset [N]$ of \"correct-so-far\" experts\n",
    "+ let $\\hat y^t$ side with the majority of expert predictions $x_i^t$, but only for $i \\in C^t$\n",
    "**Theorem**: the halving algorithm will make fewer than $\\log_2 N$ mistakes!\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Assume you have $m$ teams playing a league. On each of a sequence of rounds, some pair of teams $(i,j)$ will play each other. There is some unknown permutation $\\pi \\in S_m$ which determines the outcome of each match, so $i$ beats $j$ only when $\\pi(i) > \\pi(j)$. You're a gambler, and you want to know how to bet on who wins. What is the best algorithm to use, and how few mistakes will it make before knowing the exact ranking $\\pi$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "There are $m!$ permutations or possible rankings of the $m$ teams. Let each of these permutations $\\pi$ be an expert. Since it is given that one of these permutations are correct, we can run the Halving algorithm on these $m!$ permutations. \n",
    "\n",
    "By the Halving algorithm, this approach will make at most $\\log m! = O(m \\log m)$ mistakes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if the best expert makes one mistake?\n",
    "\n",
    "You looked at the Weighted Majority Algorithm in the reading, which can be thought of as a generalization of the Halving Algorithm when there are no perfect experts. But let's consider a simple scenario first, and try to use the Halving Algorithm again.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Assume you have $N$ experts, and each will give you a prediction on each round, and there are exactly $N$ rounds. There isn't a *perfect* expert, but there is one expert that will make exactly one mistake.\n",
    "\n",
    "Can you find an algorithm that uses the experts' advice to make predictions, and is guaranteed to make no more than $2 \\log_2 N$ mistakes? (*Hint*: Use the Halving Alg!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "For each expert $e$, make alternate experts $e'_i$ for $i \\in [N]$ such that the prediction of $e'_i$ is the same as $e$ at each round $t$, except when $t = i$. \n",
    "\n",
    "Since we are guaranteed that one expert $e$ makes exacty one mistake, then we are also guaranteed that one of it's alternates is a perfect expert, since one of them must have corrected the one mistake expert $e$ made. \n",
    "\n",
    "Since we have a perfect expert, we can run the Halving Algorithm on these $N^2$ experts, and the bound on the number of mistakes we will make is $\\log N^2 = 2 \\log N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smarter: The Weighted Majority Algorithm\n",
    "\n",
    "* Weights: $w_i^1 = 1$, for $i = 1, \\ldots, N$\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts $\\hat y^t = \\text{round}\\left(\\frac{\\sum_i w_i^t x_i^t}{\\sum_i w_i^t}\\right)$\n",
    "    + After seeing $y^t$, update weights: $w_i^{t+1} = w_i^t(1-\\epsilon)^{\\mathbb{1}[x_i^t \\ne y^t]}$\n",
    "    \n",
    "**Theorem**: Let $M_T$ be num mistakes of WMA after $T$ rounds, and let $M_T(i)$ be the number of mistakes of expert $i$. Then for every expert $i$ we have\n",
    "$$ M_T \\leq 2(1+\\epsilon)M_T(i) + \\frac{2 \\log N}{\\epsilon}$$\n",
    "\n",
    "#### Problem\n",
    "\n",
    "What is the best choice of $\\epsilon$ here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "We want to pick an $\\epsilon$ such that the bound is minimized.\n",
    "\n",
    "Let $f(\\epsilon) = 2(1 + \\epsilon)M_T(i) + \\frac{2 \\log N}{\\epsilon}$. To minimize $f(\\epsilon)$, we take find $\\epsilon$ such that $f'(\\epsilon) = 0$.\n",
    "\n",
    "$$f'(\\epsilon) = 2M_T(i) - \\frac{2 \\log N}{\\epsilon^2} = 0$$\n",
    "\n",
    "$$\\frac{\\log N}{\\epsilon^2} = M_T(i)$$\n",
    "\n",
    "$$\\epsilon = \\sqrt{\\frac{\\log N}{M_T(i)}}$$\n",
    "\n",
    "This bound is further bounded by $M_T(i^*)$ where $i^*$ is the expert that makes the fewest mistakes. Note that this requires that we know the number of mistakes the best expert makes ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Even Smarter: Randomized Weighted Majority\n",
    "\n",
    "\n",
    "* Weights: $w_i^1 = 1$, for $i = 1, \\ldots, N$\n",
    "* For $t=1,2,3,\\ldots$\n",
    "    + Expert $i$ predicts $x_i^t \\in \\{0,1\\}$, for $i=1, \\ldots, N$\n",
    "    + Algorithm predicts *randomly*: let $\\hat y^t = x_i^t$ with prob. $p_i^t = \\frac{w_i^t }{\\sum_j w_j^t}$\n",
    "    + After seeing $y^t$, update weights: $w_i^{t+1} = w_i^t(1-\\epsilon)^{\\mathbb{1}[x_i^t \\ne y^t]}$\n",
    "    \n",
    "**Theorem**: Let $M_T$ be num mistakes of RWM after $T$ rounds, and let $M_T(i)$ be the number of mistakes of expert $i$. Then for every expert $i$ we have\n",
    "$$ \\mathbb{E}[M_T] \\leq (1+\\epsilon)M_T(i) + \\frac{\\log N}{\\epsilon}$$\n",
    "\n",
    "**Corollary**: If we tune correctly, we see that $\\epsilon = \\sqrt{\\frac{\\log N}{M_T(i)}}$ where $i$ is the index of any expert (which we can choose as the best). Then we have\n",
    "$$ \\mathbb{E}[M_T] - M_T(i) \\leq 2\\sqrt{M_T(i) \\log N}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gambling, revisited\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Now assume we are in the gambling problem we had before, with $m$ teams, and on each round a pair of teams shows up to play a match. This time, however, there is some good permutation $\\pi$, but it's not perfect! There will be at most $K$ matches whose outcome doesn't following the permutation $\\pi$.\n",
    "\n",
    "What's a good algorithm for the gambler? And how many mistakes will she make along the way?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
