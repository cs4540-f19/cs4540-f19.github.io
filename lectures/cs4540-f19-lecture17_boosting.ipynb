{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# L17: Boosting + AdaBoost\n",
    "\n",
    "Jake Abernethy\n",
    "\n",
    "*Tuesday, October 29, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall: The Minimax Theorem\n",
    "\n",
    "Proved by von Neumann in the 1920s!\n",
    "\n",
    "**Theorem**: For any matrix $M \\in \\R^{n \\times m}$ we have\n",
    "$$\\min_{p \\in \\Delta_n} \\max_{j \\in [m]} p^\\top M_{:j} = \\max_{q \\in \\Delta_m} \\min_{i \\in [n]} M_{i:} q$$\n",
    "Equivalently:\n",
    "$$\\min_{p \\in \\Delta_n} \\max_{q \\in \\Delta_m} p^\\top M q = \\max_{q \\in \\Delta_m} \\min_{p \\in \\Delta_n} p^\\top M q$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Boosting (2 Class Scenario)\n",
    "\n",
    "- Assume class labels are -1 and +1.\n",
    "- The final classifier then has the form: \n",
    " - $h_T(\\mathbf{x}) = \\text{sgn}\\left(\\sum \\limits_{t = 1}^T \\alpha_t f_t(\\mathbf{x})\\right)$ where $f_1, ..., f_T$ are called base (or weak) classifiers and $\\alpha_1, ..., \\alpha_T > 0$ reflect the confidence of the various base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Base/Weak Learners\n",
    "\n",
    "- Let $(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_n, y_n)$ be the training data.\n",
    "- Let $\\mathscr{F}$ be a fixed set of classifiers called the base class.\n",
    "- A base learner for $\\mathscr{F}$ is a rule that takes as input a set of weights $\\mathbf{w} = (w_1, ..., w_n)$ such that $w_i \\geq 0, \\sum w_i = 1$, and outputs a classifier $f \\in \\mathscr{F}$ such that the weighted empirical risk $$e_w(f) = \\sum \\limits_{i = 1}^n w_i \\mathbb{1}_{\\{f(\\mathbf{x}_i) \\neq y_i\\}}$$ is (approximately) minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of Base (Weak) Learners\n",
    "\n",
    "- Decision Stumps, i.e., decision trees with depth 1\n",
    "- Decision Trees\n",
    "- Polynomial thresholds, i.e., $$f(\\vec{x}) = \\pm \\text{sign}((\\vec{w}^\\top \\vec{x})^2 - b)$$ where $b \\in \\mathbb{R}$ and $\\vec{w} \\in \\mathbb{R}^d$ is a radial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost (Adaptive Boosting)\n",
    "\n",
    "- The first concrete algorithm to successfully realize the boosting principle.\n",
    "\n",
    "<img src=\"./images/adaboost.gif\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost Algorithm\n",
    "\n",
    "An *iterative* algorithm for \"ensembling\" base learners\n",
    "\n",
    "- Input: $\\{(\\mathbf{x}_i, y_i)\\}_{i = 1}^n, T, \\mathscr{F}$, base learner\n",
    "- Initialize: $\\mathbf{w}^{1} = (\\frac{1}{n}, ..., \\frac{1}{n})$\n",
    "- For $t = 1, ..., T$\n",
    " - $\\mathbf{w}^{t} \\rightarrow \\boxed{\\text{base learner}} \\rightarrow f_t$\n",
    " - $\\alpha_t = \\frac{1}{2}\\text{ln}\\left(\\frac{1 - r_t}{r_t}\\right)$\n",
    "     - where $r_t := e_{\\mathbf{w}^t}(f_t) = \\frac 1 n \\sum \\limits_{i = 1}^n \\mathbf{w}_i \\mathbf{1}_{\\{f(\\mathbf{x}_i) \\neq y_i\\}} $\n",
    " - $w_i^{t + 1} = \\frac{\\mathbf{w}_i^t \\exp \\left(- \\alpha_ty_if_t(\\mathbf{x}_i)\\right)}{z_t}$ where $z_t$ normalizes.\n",
    "- Output: $h_T(\\mathbf{x}) = \\text{sign}\\left(\\sum \\limits_{t = 1}^T \\alpha_t f_t(\\mathbf{x})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaboost through Coordinate Descent\n",
    "\n",
    "It is often said that we can view Adaboost as \"Coordinate Descent\" on the exponential loss function.\n",
    "\n",
    "**Question**: Can you figure out what that means? Why is Adaboost doing coordinate descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 1*: You need to figure out the objective function being minimized. For simplicity, assume there are a finite number of weak learners in $\\mathscr{F}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 2*: Recall that the exponential loss function is $\\ell(h; (x,y)) = \\exp(-y h(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 3*: Let's write down the objective function being minimized. For simplicity, assume there are a finite number of weak learners in $\\mathscr{F}$, say indexed by $j=1, \\ldots, m$. Given a weight vector $\\vec{\\alpha}$,  exponential loss over the data for this $\\vec{\\alpha}$ is: \n",
    "$$\\text{Loss}(\\vec{\\alpha}) = \\sum_{i=1}^n \\exp \\left( - y_i \\left(\\sum_{j=1}^m \\alpha_j h_j(\\vec{x}_i)\\right)\\right)$$\n",
    "Coordinate descent chooses the smallest coordiante of $\\nabla L(\\vec{\\alpha})$ and updates *only this coordinate*. Which coordinate is chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
